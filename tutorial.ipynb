{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `TensorFlow`? \n",
    "The official page says:   \n",
    ">TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud\n",
    "\n",
    "pinpointing at least two reasons (or, alternatively, nowadays buzz-words) to learn it:\n",
    " - machine learning\n",
    " - open-source\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, I would like to share my basic knowledge concerning this popular framework. It is made as an assignment project for *Learning from Unstructured Data* course. Therefore, it consists of datasets and exercises used during these classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything's set up? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into Tensorflow, let's make sure we have it installed, by displaying current Tensorflow's version. From now on, we will have the library imported under the alias `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "Let's start the tutorial with understanding what is the idea of `TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is a **dataflow programming framework**.\n",
    "\n",
    "This means that we define and run a so called **computation graph**:\n",
    " - In each node, we can store operations, such as addition, multiplication. \n",
    " - On the edges, we store inputs / outputs of these functions. These can be represented as N-ranked matrices, which are called **Tensors**.\n",
    " \n",
    "The **Tensors** carry the data **flowing** in the graph.\n",
    "> Control question: Why is Tensorflow called Tensorflow? :)\n",
    "\n",
    "What is the reason for organizing data flow in such way? \n",
    "\n",
    " - For a graph given, the dependencies between nodes are described explicitly. This makes it easier to exploit parallelism and distribution across multiple devices: CPUs, GPUS and others. Imagine a visualization of a graph, so that we can see at a quick glance which nodes can be executed in parallel, as the data flows in different channels (on different edges). \n",
    " \n",
    "Great promise of optimized computations! But we also have to note, that no matter how great is the graph we design, it will remain **static at run time**. Once the graph will be running, it is impossible to change it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow exposes so called `Graph API`. We can use not only Python, but also GoLang, Java and C++ and provided library, helping to write out a graph in a special format, known as protobuf. \n",
    "> Protocol buffers are Google's language-neutral, platform-neutral, extensible **mechanism for serializing structured data** â€“ think XML, but smaller, faster, and simpler. \n",
    "\n",
    "There is also `Session API`, providing an interface to the *Tensorflow C++ Runtime*. This is where all the *heavy lifting* and *logic behind computational nodes* happens, as well as ultimate distribution of operations to be executed on the hardware.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating first graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create first simple computation graph, that will add two numbers together. Because this tutorial is built of jupyter notebook cells, we begin with a cell that will make sure we have everything cleaned up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the default graph stack \n",
    "# and reset the global default graph.\n",
    "\n",
    "# Use to play around and avoid 'dead nodes'...\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main data types of TensorFlow are:\n",
    "1. Constants\n",
    "2. Variables\n",
    "3. Placeholders\n",
    "\n",
    "To begin with, we will store two numbers in variables that are constants. We can put a `value` inside, which is of `dtype`, will not change (because is constant), and give it a `name`. For addition, we use `tf.add` function, which will return variable to store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1.0, dtype=tf.float32, name='a')\n",
    "b = tf.constant(2.0, dtype=tf.float32, name='b')\n",
    "result = tf.add(a, b, name='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"result:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `print` operation, we clearly see that we did not print the actual result. Instead, we printed information concerning tensor what will store that result. This is because we defined a graph, put some constant values, but did not let the data flow yet.\n",
    "\n",
    "Therefore, we have to make use of already mentioned `Session API` to run the graph (or we can also run part of graph, if we would like to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"Result =\", sess.run(result))\n",
    "    \n",
    "### Alternatively:\n",
    "# sess = tf.Session()\n",
    "# print(\"Result =\", sess.run(result))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A default session is defined by calling `tf.Session()`. Then, we fire up the graph and calculate the result by running `sess.run(result)`. \n",
    "\n",
    "In the cell above, there is an alternative way of more manual management of session, but it is better to make use of Python's `with` statement for safety.\n",
    "\n",
    "____________________________\n",
    "\n",
    "After this introduction, we should move on to some machine learning! Recall all computations' and matrices' of data friend, `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can start with implementing Logistic Regression in Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's use gene activity data, which is already stored in *data* directory. First, we need to read data from file, take Xs and Ys, as well as perform z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/gene_data.txt:\n",
      " 36 data records, described by 2 features\n"
     ]
    }
   ],
   "source": [
    "mat = np.loadtxt('data/gene_data.txt', delimiter='\\t', dtype=np.float32)\n",
    "Ys = mat[:, [-1]]\n",
    "Xs = mat[:, :-1]\n",
    "means = np.mean(Xs, 0)\n",
    "stdevs = np.std(Xs, 0)\n",
    "Xs = (Xs-means)/stdevs\n",
    "\n",
    "print(\"{}:\\n {} data records, described by {} features\".format('data/gene_data.txt', str(Xs.shape[0]), str(Xs.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare **constant** tensors to store data which we just read from file. \n",
    "\n",
    "Then, we need to create **variables** that will store parameters of logistic regression. These are `weights` and `bias`, that we expect to be altered by the algorithm, heading more optimal solution. In TensorFlow, variables represent a tensors whose values can be changed by running operations on them.\n",
    "> A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    " - for `weights`, we make use of `tf.random_normal` that outputs random values from a normal distribution, for given shape: (2, 1), which stands for 2 features we have, per one record of data at a time *although we will in general perform matrix operations*\n",
    " - we set initial value of `bias` to 0.0\n",
    " - we define `net` tensor, using `tf.matmul` and `tf.add` for calculating weighted linear combination of the input, with a bias\n",
    " - and `output` tensor, which we obtain by applying `sigmoid` function for the tensor above\n",
    " \n",
    "Looks like we have all components of logistic regression - we can input some data and receive some output prediction - but we still need another ones to make the learning possible. Therefore, we further define:\n",
    "\n",
    " - logistic `cost` function: notice that we use tensors joined together with arithmetical operations, that also results in a suitable tensor; applying `tf.reduce_mean` is equivalent to `np.mean` - simply averaging over all elements\n",
    " - an `optimizer`, which is an instance of `GradientDescentOptimizer` class, that implements algorithm specified in its name; we set its `learning_rate` to 0.1\n",
    " - and `training_op`, result of calling `minimize` function which is a method of the class used above\n",
    " \n",
    "At a first glance, it seems as we didn't need to know much about Gradient Descent algorithm, as it is already implemented and easy to use. This is one of the advantages TensorFlow has, that does not concern performance and low-level characteristics, but rather convenience and flexibility at the same time.\n",
    "\n",
    ">What happens in `.minimize(loss=cost)`?\n",
    "<br>This function first computes gradients of all variables provided. TensorFlow automatically assumes that user-defined variables will be *trained*. Indeed, we defined complete graph, as well as `cost` that depends on `output`, and so on... \n",
    "For each variable, the gradient can be another Tensor, or can represent None when no gradient exists. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # :) \n",
    "\n",
    "X = tf.constant(Xs, name=\"X\")\n",
    "y = tf.constant(Ys, name=\"y\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal(shape=(2, 1)), name=\"weights\")\n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "net = tf.add(tf.matmul(X, weights), bias, name=\"net\")\n",
    "output = tf.nn.sigmoid(net, name=\"output\")\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(output) + (1-y) * (tf.log(1-output)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\n",
    "training_op = optimizer.minimize(loss=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all set and will run the graph 20 times. This means that we will calculate current loss 20 times, calculating the gradient each time and adjusting the weights based on learning rate.\n",
    "\n",
    "Because as the documentation states, variable initializers must be run explicitly before other operations (and we surely have declared some), we will use convenient one-line method that will do the trick. This is included in the first line below.\n",
    "\n",
    "Notice how we pass tensors as parameters to the `sess.run` method, so that they are executed and returned. We skip the first variable returned by `.minimize` by a `_` variable, for it will not output any number that is in our interest; we rather want it to perform calculations and result in weights changing. \n",
    "\n",
    "Running the cell below will train our model 20 times, and print current logistic loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.5519187\n",
      "2 : 0.50460744\n",
      "3 : 0.46623346\n",
      "4 : 0.43458194\n",
      "5 : 0.40806475\n",
      "6 : 0.38553414\n",
      "7 : 0.36614892\n",
      "8 : 0.3492829\n",
      "9 : 0.33446258\n",
      "10 : 0.3213244\n",
      "11 : 0.30958506\n",
      "12 : 0.29902112\n",
      "13 : 0.28945413\n",
      "14 : 0.28073993\n",
      "15 : 0.272761\n",
      "16 : 0.26542044\n",
      "17 : 0.25863796\n",
      "18 : 0.2523462\n",
      "19 : 0.24648847\n",
      "20 : 0.2410166\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20):\n",
    "        _, current_loss = sess.run([training_op, cost])\n",
    "        print(\"{} : {}\".format(str(epoch+1), str(current_loss)))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be happy to see that after each epoch, the loss value tends to decrease, so our graph is working. \n",
    "\n",
    "But we want something more from TensorFlow, and this 'something' is probably *neural networks*. How would a very simple neural network look like?\n",
    "\n",
    "We already have an example with logistic regression, and we can think of it as a one layer neural network. Therefore, the example of single neuron that represents OR should be from now understandable. It only differs from the Logistic Regression example by the objective function, which is not a logistic loss, but a mean squared error. It is not a big change at all; we take same tensors to determine the value of loss, but apply different operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One neuron that learns OR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : mse=0.24710208\n",
      "100 : mse=0.1285279\n",
      "200 : mse=0.106404796\n",
      "300 : mse=0.090100385\n",
      "400 : mse=0.07716124\n",
      "500 : mse=0.06685236\n",
      "600 : mse=0.05856094\n",
      "700 : mse=0.051808838\n",
      "800 : mse=0.046241608\n",
      "900 : mse=0.04159805\n",
      "1000 : mse=0.037684083\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Pairs of inputs (possible combinations)\n",
    "Xs = np.array([(0,0), (0,1), (1,0), (1,1)])\n",
    "# Results on applying OR function on the pairs above\n",
    "Ys = np.array([0, 1, 1, 1])\n",
    "\n",
    "X = tf.constant(Xs.astype(np.float32), name=\"X\")\n",
    "y = tf.constant(Ys.astype(np.float32).reshape((-1,1)), name=\"y\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal((2,1)), name=\"weights\")\n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "net = tf.add(tf.matmul(X, weights), bias, name=\"net\")\n",
    "output = tf.nn.sigmoid(net, name=\"output\")\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(y-output),name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(loss=mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1001):\n",
    "        _, current_mse = sess.run([training_op, mse])\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"{} : mse={}\".format(str(epoch), str(current_mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Troubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of having only one neuron is that we can question it by a popular *counter-argument* - can one neuron learn XOR function? \n",
    "> No, because one neuron is just a linear classifier, and we can not separate XOR with only one line to split the space\n",
    "\n",
    "Therefore, we will now implement a small neural network with one hidden layer, that can solve the XOR problem, and see how we can connect more units together.\n",
    "\n",
    "\n",
    "\n",
    "More neurons and layers naturally imply that we will need more tensors. It is clearly visible in the code cell below - we define weights, bias, net and output tensors for each of the neurons. Two integer numbers are added to variables' names', first of them indicates number of layer (hidden has number 1, the next one is output layer and has number 2), and the next number is for numbering a neuron within its layer.\n",
    "\n",
    "> Notice how we multiply tensor `X` with both `weights11` and `weights12` for each neuron, subsequently adding bias.\n",
    "<br>To join outputs of two neurons in hidden layer together, we make use of `tf.concat` function, that concatenates two hidden layer outputs along columns (it is specified by axis=1, as we index axis starting from 0 - rows)\n",
    "\n",
    "It wasn't that hard, many things look similar to what we already know! Run the cell below and see how it works, will those neurons learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : mse=0.25095218\n",
      "2000 : mse=0.24822871\n",
      "4000 : mse=0.23079756\n",
      "6000 : mse=0.10747802\n",
      "8000 : mse=0.02328165\n",
      "10000 : mse=0.010499645\n",
      "12000 : mse=0.006452598\n",
      "14000 : mse=0.0045708744\n",
      "16000 : mse=0.0035062141\n",
      "18000 : mse=0.0028287258\n",
      "20000 : mse=0.0023626816\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Pairs of inputs (possible combinations)\n",
    "X = tf.constant(np.array([(0,0),(0,1),(1,0),(1,1)]).astype(np.float32), name=\"X\")\n",
    "# Results on applying XOR function on the pairs above\n",
    "y = tf.constant(np.array([0,1,1,0]).astype(np.float32).reshape((-1,1)), name=\"y\")\n",
    "\n",
    "# HIDDEN LAYER:\n",
    "#     1st neuron:\n",
    "weights11 = tf.Variable(tf.random_normal((2, 1)), name=\"weights11\")\n",
    "bias11 = tf.Variable(0.0, name=\"bias11\")\n",
    "net11 = tf.add(tf.matmul(X, weights11), bias11, name=\"net11\")\n",
    "output11 = tf.nn.sigmoid(net11, name=\"output11\")\n",
    "\n",
    "#     2nd neuron:\n",
    "weights12 = tf.Variable(tf.random_normal((2, 1)), name=\"weights12\")\n",
    "bias12 = tf.Variable(0.0, name=\"bias12\")\n",
    "net12 = tf.add(tf.matmul(X, weights12), bias12, name=\"net12\")\n",
    "output12 = tf.nn.sigmoid(net12, name=\"output12\")\n",
    "\n",
    "# OUTPUT LAYER:\n",
    "#     just one neuron:\n",
    "weights21 = tf.Variable(tf.random_normal((2, 1)), name=\"weights21\")\n",
    "bias21 = tf.Variable(0.0, name=\"bias21\")\n",
    "\n",
    "input21 = tf.concat([output11, output12], axis=1)\n",
    "\n",
    "net21 = tf.add(tf.matmul(input21, weights21), bias21, name=\"net12\")\n",
    "output = tf.nn.sigmoid(net21, name=\"output\")\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(y-output),name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(loss=mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        _, cost = sess.run([training_op, mse])\n",
    "        if epoch % 2000 == 0:\n",
    "            print(\"{} : mse={}\".format(str(epoch), str(cost)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we see that the network is learning, but the cell above is made up of 42 lines. For only four different pairs of binary values. We declared tensors that store each neuron's parameters separately. And used all examples existing at a time, each epoch...\n",
    "\n",
    "This won't scale too good if the problem gets complicated. Thus, time to move to such problem! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is a very famous set of images of handwritten digits. It provides 60 000 examples in a training set, and 10 000 examples in a test set, all of which have been size-normalized and centered in a fixed-size image. Because of that, we can apply it easly in this tutorial, without spending time on preprocessing and formatting the data.\n",
    "\n",
    "There is a function already provided that will prepare all the data we need. If needed, the dataset will be downloaded from the Internet, and then it will be cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "mnist  = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset we loaded has matching sizes of training and test set, and that each image's size is 28x28 pixels in grayscale. \n",
    "\n",
    "Let's keep `test_images` data for the very end, final evaluation of the model we are about to create. Thus, we need to split entire training set to training and validation set. Say we need only 10.000 for validation and keep the rest for training.\n",
    "\n",
    "Another thing that we need to do with our data is altering the dimensions. We are going to create a fully-connected layer, so that we can't really input 2D image into it. To achieve that, we will use:\n",
    " - `.reshape(-1, 28*28)`, in which we specified expected shape as `-1, 28*28`. What does it mean? `28*28` indicates that we want to create a 1D vector out of 2D image matrix. We further use `-1` that will tell the function to infer the remaining dimensions, in case we already forgot how many image samples we have ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = train_images[:50000].reshape(-1, 28*28)\n",
    "train_labels = train_labels[:50000]\n",
    "\n",
    "valid_images = train_images[50000:].reshape(-1, 28*28)\n",
    "valid_labels = train_labels[50000:]\n",
    "\n",
    "test_images = test_images.reshape(-1, 28*28)\n",
    "\n",
    "num_features = train_images.shape[1]\n",
    "num_labels = len(np.unique(train_labels))\n",
    "\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new size of `train_images` shows that we managed to *flatten* the images into 1D vectors, with 784 elements each. For readability, we also defined `num_features` variable, that should actually store number 784, for this is the amount of features that describe every single image. We also easly determine `num_labels` by counting all unique labels that can be found in `train_images` set of data.\n",
    "\n",
    "Now, based on the previous conclusions about too much lines of code, let's write a function that will be able to create one layer of neurons. We would like to tell such function:\n",
    "\n",
    " - what is the input tensor that will provide thata to the layer we are creating,\n",
    " - how many neurons we would like to have in a layer,\n",
    " - which activation function we want to apply on the neurons, since we know many other than *sigmoid*,\n",
    " - and preferably give this layer a *name*, as we already saw that we can name the tensors\n",
    " \n",
    "We mentioned above that it would be convenient to change each layer's activation function via the parameter. Well, in TensorFlow, there are many activation functions ready to use. You can check out all the possibities __[here](https://www.tensorflow.org/api_docs/python/tf/keras/activations)__.\n",
    "\n",
    "Let us see the implementation of `layer()` function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(inputs, neurons, name, activation_f=None):\n",
    "    with tf.name_scope(name):\n",
    "        weights = tf.Variable(tf.random_normal(\n",
    "            (int(inputs.shape[1]), neurons) ), name=\"weights\")\n",
    "        bias = tf.Variable(tf.zeros(neurons), name=\"bias\")\n",
    "        net = tf.add(tf.matmul(inputs, weights), bias, name=\"net\")\n",
    "        if activation_f:\n",
    "            try:\n",
    "                return activation_f(net)\n",
    "            except:\n",
    "                print(\"Failed to apply activation_f\")\n",
    "                return net\n",
    "        else:\n",
    "            return net\n",
    "        \n",
    "def layer(inputs, neurons, name, activation_f=None):\n",
    "    with tf.name_scope(name):\n",
    "        weights = tf.Variable(tf.random_normal(\n",
    "            (int(inputs.shape[1]), neurons) ), name=\"weights\")\n",
    "        bias = tf.Variable(tf.zeros([neurons]), name=\"bias\")\n",
    "        net = tf.add(tf.matmul(inputs, weights), bias, name=\"net\")\n",
    "        if activation_f:\n",
    "            if activation_f == 'sigmoid':\n",
    "                return tf.nn.sigmoid(net)\n",
    "            elif activation_f == 'relu':\n",
    "                return tf.nn.relu(net)\n",
    "            elif activation_f == 'relu6':\n",
    "                return tf.nn.relu6(net)\n",
    "            elif activation_f == 'leaky_relu':\n",
    "                return tf.nn.leaky_relu(net)\n",
    "            elif activation_f == 'elu':\n",
    "                return tf.nn.elu(net)\n",
    "            else:\n",
    "                print(\"Unknown activation function name\")\n",
    "                return net\n",
    "        else:\n",
    "            return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "features = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "labels = tf.placeholder(tf.int64, shape=(None))\n",
    "one_hot_labels = tf.one_hot(labels, depth = num_labels,\n",
    "                            on_value = 1.0, off_value = 0.0, axis = -1)\n",
    "\n",
    "#network\n",
    "layer1 = layer(features, 30, 'layer1', activation_f='sigmoid')\n",
    "layer2 = layer(layer1, 20,'layer2', activation_f='sigmoid')\n",
    "layer3 = layer(layer2, 10,'layer3')\n",
    "\n",
    "last_layer = layer3\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(\n",
    "    onehot_labels=one_hot_labels,\n",
    "    logits=last_layer))\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.25\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "class_probabilities = tf.nn.softmax(last_layer)\n",
    "\n",
    "prediction = tf.argmax(class_probabilities, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, prediction), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AbortedError",
     "evalue": "Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:136\n\t [[Node: Softmax = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](layer3/net, DMT/_0)]]\n\nCaused by op 'Softmax', defined at:\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-0d4043d1bd3c>\", line 24, in <module>\n    class_probabilities = tf.nn.softmax(last_layer)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n    return compute_op(logits, name=name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7137, in softmax\n    \"Softmax\", logits=logits, name=name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nAbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:136\n\t [[Node: Softmax = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](layer3/net, DMT/_0)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbortedError\u001b[0m: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:136\n\t [[Node: Softmax = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](layer3/net, DMT/_0)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-acf0a8956356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mvalidation_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mvalid_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}, accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbortedError\u001b[0m: Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:136\n\t [[Node: Softmax = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](layer3/net, DMT/_0)]]\n\nCaused by op 'Softmax', defined at:\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-0d4043d1bd3c>\", line 24, in <module>\n    class_probabilities = tf.nn.softmax(last_layer)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n    return compute_op(logits, name=name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7137, in softmax\n    \"Softmax\", logits=logits, name=name)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/jells123/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nAbortedError (see above for traceback): Operation received an exception:Status: 3, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_softmax_op.cc:136\n\t [[Node: Softmax = _MklSoftmax[T=DT_FLOAT, _kernel=\"MklOp\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](layer3/net, DMT/_0)]]\n"
     ]
    }
   ],
   "source": [
    "shuffled = np.array(range(train_images.shape[0]))\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run(session=session)\n",
    "    for epoch in range(num_epochs):\n",
    "        offset = 0\n",
    "        np.random.shuffle(shuffled)\n",
    "        while offset<train_images.shape[0]:\n",
    "            batch_images = train_images[shuffled[offset:offset+batch_size]]\n",
    "            batch_labels = train_labels[shuffled[offset:offset+batch_size]]\n",
    "            batch_dict = {features : batch_images, labels : batch_labels}\n",
    "            session.run([optimizer], feed_dict=batch_dict)\n",
    "            offset += batch_size\n",
    "\n",
    "        validation_dict = {features : valid_images, labels : valid_labels}\n",
    "        acc = session.run([accuracy], feed_dict=validation_dict)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch {}, accuracy: {}\".format(str(epoch), str(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "    https://medium.com/@ouwenhuang/tensorflow-graphs-are-just-protobufs-9df51fc7d08d\n",
    "    https://medium.com/themlblog/getting-started-with-tensorflow-constants-variables-placeholders-and-sessions-80900727b489\n",
    "    https://developers.google.com/protocol-buffers/\n",
    "    http://yann.lecun.com/exdb/mnist/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
