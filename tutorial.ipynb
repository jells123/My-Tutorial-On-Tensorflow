{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `TensorFlow`? \n",
    "The official page says:   \n",
    ">TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud\n",
    "\n",
    "pinpointing at least two reasons (or, alternatively, nowadays buzz-words) to learn it:\n",
    " - machine learning\n",
    " - open-source\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, I would like to share my basic knowledge concerning this popular framework. It is made as an assignment project for *Learning from Unstructured Data* course. Therefore, it consists of datasets and exercises used during these classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything's set up? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into Tensorflow, let's make sure we have it installed, by displaying current Tensorflow's version. From now on, we will have the library imported under the alias `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "Let's start the tutorial with understanding what is the idea of `TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is a **dataflow programming framework**.\n",
    "\n",
    "This means that we define and run a so called **computation graph**:\n",
    " - In each node, we can store operations, such as addition, multiplication. \n",
    " - On the edges, we store inputs / outputs of these functions. These can be represented as N-ranked matrices, which are called **Tensors**.\n",
    " \n",
    "The **Tensors** carry the data **flowing** in the graph.\n",
    "> Control question: Why is Tensorflow called Tensorflow? :)\n",
    "\n",
    "What is the reason for organizing data flow in such way? \n",
    "\n",
    " - For a graph given, the dependencies between nodes are described explicitly. This makes it easier to exploit parallelism and distribution across multiple devices: CPUs, GPUS and others. Imagine a visualization of a graph, so that we can see at a quick glance which nodes can be executed in parallel, as the data flows in different channels (on different edges). \n",
    " \n",
    "Great promise of optimized computations! But we also have to note, that no matter how great is the graph we design, it will remain **static at run time**. Once the graph will be running, it is impossible to change it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow exposes so called `Graph API`. We can use not only Python, but also GoLang, Java and C++ and provided library, helping to write out a graph in a special format, known as protobuf. \n",
    "> Protocol buffers are Google's language-neutral, platform-neutral, extensible **mechanism for serializing structured data** â€“ think XML, but smaller, faster, and simpler. \n",
    "\n",
    "There is also `Session API`, providing an interface to the *Tensorflow C++ Runtime*. This is where all the *heavy lifting* and *logic behind computational nodes* happens, as well as ultimate distribution of operations to be executed on the hardware.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating first graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create first simple computation graph, that will add two numbers together. Because this tutorial is built of jupyter notebook cells, we begin with a cell that will make sure we have everything cleaned up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the default graph stack \n",
    "# and reset the global default graph.\n",
    "\n",
    "# Use to play around and avoid 'dead nodes'...\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main data types of TensorFlow are:\n",
    "1. Constants\n",
    "2. Variables\n",
    "3. Placeholders\n",
    "\n",
    "To begin with, we will store two numbers in variables that are constants. We can put a `value` inside, which is of `dtype`, will not change (because is constant), and give it a `name`. For addition, we use `tf.add` function, which will return variable to store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1.0, dtype=tf.float32, name='a')\n",
    "b = tf.constant(2.0, dtype=tf.float32, name='b')\n",
    "result = tf.add(a, b, name='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"result:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `print` operation, we clearly see that we did not print the actual result. Instead, we printed information concerning tensor what will store that result. This is because we defined a graph, put some constant values, but did not let the data flow yet.\n",
    "\n",
    "Therefore, we have to make use of already mentioned `Session API` to run the graph (or we can also run part of graph, if we would like to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"Result =\", sess.run(result))\n",
    "    \n",
    "### Alternatively:\n",
    "# sess = tf.Session()\n",
    "# print(\"Result =\", sess.run(result))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A default session is defined by calling `tf.Session()`. Then, we fire up the graph and calculate the result by running `sess.run(result)`. \n",
    "\n",
    "In the cell above, there is an alternative way of more manual management of session, but it is better to make use of Python's `with` statement for safety.\n",
    "\n",
    "____________________________\n",
    "\n",
    "After this introduction, we should move on to some machine learning! Recall all computations' and matrices' of data friend, `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can start with implementing Logistic Regression in Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's use gene activity data, which is already stored in *data* directory. First, we need to read data from file, take Xs and Ys, as well as perform z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/gene_data.txt:\n",
      " 36 data records, described by 2 features\n"
     ]
    }
   ],
   "source": [
    "mat = np.loadtxt('data/gene_data.txt', delimiter='\\t', dtype=np.float32)\n",
    "Ys = mat[:, [-1]]\n",
    "Xs = mat[:, :-1]\n",
    "means = np.mean(Xs, 0)\n",
    "stdevs = np.std(Xs, 0)\n",
    "Xs = (Xs-means)/stdevs\n",
    "\n",
    "print(\"{}:\\n {} data records, described by {} features\".format('data/gene_data.txt', str(Xs.shape[0]), str(Xs.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare **constant** tensors to store data which we just read from file. \n",
    "\n",
    "Then, we need to create **variables** that will store parameters of logistic regression. These are `weights` and `bias`, that we expect to be altered by the algorithm, heading more optimal solution. In TensorFlow, variables represent a tensors whose values can be changed by running operations on them.\n",
    "> A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    " - for `weights`, we make use of `tf.random_normal` that outputs random values from a normal distribution, for given shape: (2, 1), which stands for 2 features we have, per one record of data at a time *although we will in general perform matrix operations*\n",
    " - we set initial value of `bias` to 0.0\n",
    " - we define `net` tensor, using `tf.matmul` and `tf.add` for calculating weighted linear combination of the input, with a bias\n",
    " - and `output` tensor, which we obtain by applying `sigmoid` function for the tensor above\n",
    " \n",
    "Looks like we have all components of logistic regression - we can input some data and receive some output prediction - but we still need another ones to make the learning possible. Therefore, we further define:\n",
    "\n",
    " - logistic `cost` function: notice that we use tensors joined together with arithmetical operations, that also results in a suitable tensor; applying `tf.reduce_mean` is equivalent to `np.mean` - simply averaging over all elements\n",
    " - an `optimizer`, which is an instance of `GradientDescentOptimizer` class, that implements algorithm specified in its name; we set its `learning_rate` to 0.1\n",
    " - and `training_op`, result of calling `minimize` function which is a method of the class used above\n",
    " \n",
    "At a first glance, it seems as we didn't need to know much about Gradient Descent algorithm, as it is already implemented and easy to use. This is one of the advantages TensorFlow has, that does not concern performance and low-level characteristics, but rather convenience and flexibility at the same time.\n",
    "\n",
    ">What happens in `.minimize(loss=cost)`?\n",
    "<br>This function first computes gradients of all variables provided. TensorFlow automatically assumes that user-defined variables will be *trained*. Indeed, we defined complete graph, as well as `cost` that depends on `output`, and so on... \n",
    "For each variable, the gradient can be another Tensor, or can represent None when no gradient exists. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # :) \n",
    "\n",
    "X = tf.constant(Xs, name=\"X\")\n",
    "y = tf.constant(Ys, name=\"y\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal(shape=(2, 1)), name=\"weights\")\n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "net = tf.add(tf.matmul(X, weights), bias, name=\"net\")\n",
    "output = tf.nn.sigmoid(net, name=\"output\")\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(output) + (1-y) * (tf.log(1-output)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\n",
    "training_op = optimizer.minimize(loss=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all set and will run the graph 20 times. This means that we will calculate current loss 20 times, calculating the gradient each time and adjusting the weights based on learning rate.\n",
    "\n",
    "Because as the documentation states, variable initializers must be run explicitly before other operations (and we surely have declared some), we will use convenient one-line method that will do the trick. This is included in the first line below.\n",
    "\n",
    "Notice how we pass tensors as parameters to the `sess.run` method, so that they are executed and returned. We skip the first variable returned by `.minimize` by a `_` variable, for it will not output any number that is in our interest; we rather want it to perform calculations and result in weights changing. \n",
    "\n",
    "Running the cell below will train our model 20 times, and print current logistic loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.442709\n",
      "2 : 0.41387594\n",
      "3 : 0.38964933\n",
      "4 : 0.3690012\n",
      "5 : 0.35118067\n",
      "6 : 0.3356298\n",
      "7 : 0.32192647\n",
      "8 : 0.3097464\n",
      "9 : 0.29883677\n",
      "10 : 0.28899753\n",
      "11 : 0.28006873\n",
      "12 : 0.27192074\n",
      "13 : 0.2644478\n",
      "14 : 0.2575625\n",
      "15 : 0.251192\n",
      "16 : 0.24527533\n",
      "17 : 0.2397609\n",
      "18 : 0.23460478\n",
      "19 : 0.2297694\n",
      "20 : 0.22522235\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20):\n",
    "        _, current_loss = sess.run([training_op, cost])\n",
    "        print(\"{} : {}\".format(str(epoch+1), str(current_loss)))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be happy to see that after each epoch, the loss value tends to decrease, so our graph is working. \n",
    "\n",
    "But we want something more from TensorFlow, and this 'something' is probably *neural networks*. How would a very simple neural network look like?\n",
    "\n",
    "We already have an example with logistic regression, and we can think of it as a one layer neural network. Therefore, the example of single neuron that represents OR should be from now understandable. It only differs from the Logistic Regression example by the objective function, which is not a logistic loss, but a mean squared error. It is not a big change at all; we take same tensors to determine the value of loss, but apply different operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One neuron that learns OR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : mse=0.39391166\n",
      "100 : mse=0.15326461\n",
      "200 : mse=0.12623389\n",
      "300 : mse=0.10645351\n",
      "400 : mse=0.09030949\n",
      "500 : mse=0.07739184\n",
      "600 : mse=0.067077994\n",
      "700 : mse=0.058769055\n",
      "800 : mse=0.051994704\n",
      "900 : mse=0.046404857\n",
      "1000 : mse=0.041740462\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Pairs of inputs (possible combinations)\n",
    "Xs = np.array([(0,0), (0,1), (1,0), (1,1)])\n",
    "# Results on applying OR function on the pairs above\n",
    "Ys = np.array([0, 1, 1, 1])\n",
    "\n",
    "X = tf.constant(Xs.astype(np.float32), name=\"X\")\n",
    "y = tf.constant(Ys.astype(np.float32).reshape((-1,1)), name=\"y\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal((2,1)), name=\"weights\")\n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "net = tf.add(tf.matmul(X, weights), bias, name=\"net\")\n",
    "output = tf.nn.sigmoid(net, name=\"output\")\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(y-output),name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(loss=mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1001):\n",
    "        _, current_mse = sess.run([training_op, mse])\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"{} : mse={}\".format(str(epoch), str(current_mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Troubles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of having only one neuron is that we can question it by a popular *counter-argument* - can one neuron learn XOR function? \n",
    "> No, because one neuron is just a linear classifier, and we can not separate XOR with only one line to split the space\n",
    "\n",
    "Therefore, we will now implement a small neural network with one hidden layer, that can solve the XOR problem, and see how we can connect more units together.\n",
    "\n",
    "\n",
    "\n",
    "More neurons and layers naturally imply that we will need more tensors. It is clearly visible in the code cell below - we define weights, bias, net and output tensors for each of the neurons. Two integer numbers are added to variables' names', first of them indicates number of layer (hidden has number 1, the next one is output layer and has number 2), and the next number is for numbering a neuron within its layer.\n",
    "\n",
    "> Notice how we multiply tensor `X` with both `weights11` and `weights12` for each neuron, subsequently adding bias.\n",
    "<br>To join outputs of two neurons in hidden layer together, we make use of `tf.concat` function, that concatenates two hidden layer outputs along columns (it is specified by axis=1, as we index axis starting from 0 - rows)\n",
    "\n",
    "It wasn't that hard, many things look similar to what we already know! Run the cell below and see how it works, will those neurons learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : mse=0.2536455\n",
      "2000 : mse=0.24906723\n",
      "4000 : mse=0.23994184\n",
      "6000 : mse=0.18723357\n",
      "8000 : mse=0.1490287\n",
      "10000 : mse=0.13765413\n",
      "12000 : mse=0.13320681\n",
      "14000 : mse=0.13096595\n",
      "16000 : mse=0.12964542\n",
      "18000 : mse=0.12878445\n",
      "20000 : mse=0.12818265\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Pairs of inputs (possible combinations)\n",
    "X = tf.constant(np.array([(0,0),(0,1),(1,0),(1,1)]).astype(np.float32), name=\"X\")\n",
    "# Results on applying XOR function on the pairs above\n",
    "y = tf.constant(np.array([0,1,1,0]).astype(np.float32).reshape((-1,1)), name=\"y\")\n",
    "\n",
    "# HIDDEN LAYER:\n",
    "#     1st neuron:\n",
    "weights11 = tf.Variable(tf.random_normal((2, 1)), name=\"weights11\")\n",
    "bias11 = tf.Variable(0.0, name=\"bias11\")\n",
    "net11 = tf.add(tf.matmul(X, weights11), bias11, name=\"net11\")\n",
    "output11 = tf.nn.sigmoid(net11, name=\"output11\")\n",
    "\n",
    "#     2nd neuron:\n",
    "weights12 = tf.Variable(tf.random_normal((2, 1)), name=\"weights12\")\n",
    "bias12 = tf.Variable(0.0, name=\"bias12\")\n",
    "net12 = tf.add(tf.matmul(X, weights12), bias12, name=\"net12\")\n",
    "output12 = tf.nn.sigmoid(net12, name=\"output12\")\n",
    "\n",
    "# OUTPUT LAYER:\n",
    "#     just one neuron:\n",
    "weights21 = tf.Variable(tf.random_normal((2, 1)), name=\"weights21\")\n",
    "bias21 = tf.Variable(0.0, name=\"bias21\")\n",
    "\n",
    "input21 = tf.concat([output11, output12], axis=1)\n",
    "\n",
    "net21 = tf.add(tf.matmul(input21, weights21), bias21, name=\"net12\")\n",
    "output = tf.nn.sigmoid(net21, name=\"output\")\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(y-output),name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "training_op = optimizer.minimize(loss=mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20001):\n",
    "        _, cost = sess.run([training_op, mse])\n",
    "        if epoch % 2000 == 0:\n",
    "            print(\"{} : mse={}\".format(str(epoch), str(cost)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we see that the network is learning, but the cell above is made up of 42 lines. For only four different pairs of binary values. This won't scale too good if the problem gets complicated...  Thus, time to move to such problem! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "    https://medium.com/@ouwenhuang/tensorflow-graphs-are-just-protobufs-9df51fc7d08d\n",
    "    https://medium.com/themlblog/getting-started-with-tensorflow-constants-variables-placeholders-and-sessions-80900727b489\n",
    "    https://developers.google.com/protocol-buffers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
