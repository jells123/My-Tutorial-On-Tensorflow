{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `TensorFlow`? \n",
    "The official page says:   \n",
    ">TensorFlow is an open-source machine learning library for research and production. TensorFlow offers APIs for beginners and experts to develop for desktop, mobile, web, and cloud\n",
    "\n",
    "pinpointing at least two reasons (or, alternatively, nowadays buzz-words) to learn it:\n",
    " - machine learning\n",
    " - open-source\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, I would like to share my basic knowledge concerning this popular framework. It is made as an assignment project for *Learning from Unstructured Data* course. Therefore, it consists of datasets and exercises used during these classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything's set up? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before digging into Tensorflow, let's make sure we have it installed, by displaying current Tensorflow's version. From now on, we will have the library imported under the alias `tf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "Let's start the tutorial with understanding what is the idea of `TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is a **dataflow programming framework**.\n",
    "\n",
    "This means that we define and run a so called **computation graph**:\n",
    " - In each node, we can store operations, such as addition, multiplication. \n",
    " - On the edges, we store inputs / outputs of these functions. These can be represented as N-ranked matrices, which are called **Tensors**.\n",
    " \n",
    "The **Tensors** carry the data **flowing** in the graph.\n",
    "> Control question: Why is Tensorflow called Tensorflow? :)\n",
    "\n",
    "What is the reason for organizing data flow in such way? \n",
    "\n",
    " - For a graph given, the dependencies between nodes are described explicitly. This makes it easier to exploit parallelism and distribution across multiple devices: CPUs, GPUS and others. Imagine a visualization of a graph, so that we can see at a quick glance which nodes can be executed in parallel, as the data flows in different channels (on different edges). \n",
    " \n",
    "Great promise of optimized computations! But we also have to note, that no matter how great is the graph we design, it will remain **static at run time**. Once the graph will be running, it is impossible to change it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow exposes so called `Graph API`. We can use not only Python, but also GoLang, Java and C++ and provided library, helping to write out a graph in a special format, known as protobuf. \n",
    "> Protocol buffers are Google's language-neutral, platform-neutral, extensible **mechanism for serializing structured data** â€“ think XML, but smaller, faster, and simpler. \n",
    "\n",
    "There is also `Session API`, providing an interface to the *Tensorflow C++ Runtime*. This is where all the *heavy lifting* and *logic behind computational nodes* happens, as well as ultimate distribution of operations to be executed on the hardware.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating first graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create first simple computation graph, that will add two numbers together. Because this tutorial is built of jupyter notebook cells, we begin with a cell that will make sure we have everything cleaned up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the default graph stack \n",
    "# and reset the global default graph.\n",
    "\n",
    "# Use to play around and avoid 'dead nodes'...\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main data types of TensorFlow are:\n",
    "1. Constants\n",
    "2. Variables\n",
    "3. Placeholders\n",
    "\n",
    "To begin with, we will store two numbers in variables that are constants. We can put a `value` inside, which is of `dtype`, will not change (because is constant), and give it a `name`. For addition, we use `tf.add` function, which will return variable to store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant(1.0, dtype=tf.float32, name='a')\n",
    "b = tf.constant(2.0, dtype=tf.float32, name='b')\n",
    "result = tf.add(a, b, name='result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"result:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `print` operation, we clearly see that we did not print the actual result. Instead, we printed information concerning tensor what will store that result. This is because we defined a graph, put some constant values, but did not let the data flow yet.\n",
    "\n",
    "Therefore, we have to make use of already mentioned `Session API` to run the graph (or we can also run part of graph, if we would like to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = 3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"Result =\", sess.run(result))\n",
    "    \n",
    "### Alternatively:\n",
    "# sess = tf.Session()\n",
    "# print(\"Result =\", sess.run(result))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A default session is defined by calling `tf.Session()`. Then, we fire up the graph and calculate the result by running `sess.run(result)`. \n",
    "\n",
    "In the cell above, there is an alternative way of more manual management of session, but it is better to make use of Python's `with` statement for safety.\n",
    "\n",
    "____________________________\n",
    "\n",
    "After this introduction, we should move on to some machine learning! Recall all computations' and matrices' of data friend, `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can start with implementing Logistic Regression in Tensorflow.\n",
    "\n",
    "Let's use gene activity data, which is already stored in *data* directory. First, we need to read data from file, take Xs and Ys, as well as perform z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/gene_data.txt:\n",
      " 36 data records, described by 2 features\n"
     ]
    }
   ],
   "source": [
    "mat = np.loadtxt('data/gene_data.txt', delimiter='\\t', dtype=np.float32)\n",
    "Ys = mat[:, [-1]]\n",
    "Xs = mat[:, :-1]\n",
    "means = np.mean(Xs, 0)\n",
    "stdevs = np.std(Xs, 0)\n",
    "Xs = (Xs-means)/stdevs\n",
    "\n",
    "print(\"{}:\\n {} data records, described by {} features\".format('data/gene_data.txt', str(Xs.shape[0]), str(Xs.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare **constant** tensors to store data which we just read from file. \n",
    "\n",
    "Then, we need to create **variables** that will store parameters of logistic regression. These are `weights` and `bias`, that we expect to be altered by the algorithm, heading more optimal solution. In TensorFlow, variables represent a tensors whose values can be changed by running operations on them.\n",
    "> A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    " - for `weights`, we make use of `tf.random_normal` that outputs random values from a normal distribution, for given shape: (2, 1)\n",
    " - we set initial value of `bias` to 0.0\n",
    " - we define `net` tensor, using `tf.matmul` and `tf.add` for calculating weighted linear combination of the input, with a bias\n",
    " - and `output` tensor, which we obtain by applying `sigmoid` function for the tensor above\n",
    " \n",
    "Looks like we have all components of logistic regression - we can input some data and receive some output prediction - but we still need another ones to make the learning possible. Therefore, we further define:\n",
    "\n",
    " - logistic `cost` function: notice that we use tensors joined together with arithmetical operations, that also results in a suitable tensor; applying `tf.reduce_mean` is equivalent to `np.mean` - simply averaging over all elements\n",
    " - an `optimizer`, which is an instance of `GradientDescentOptimizer` class, that implements algorithm specified in its name; we set its `learning_rate` to 0.1\n",
    " - and `training_op`, result of calling `minimize` function which is a method of the class used above\n",
    " \n",
    "At a first glance, it seems as we didn't need to know much about Gradient Descent algorithm, as it is already implemented and easy to use. This is one of the advantages TensorFlow has, that does not concern performance and low-level characteristics, but rather convenience and flexibility at the same time.\n",
    "\n",
    ">What happens in `.minimize(loss=cost)`?\n",
    "<br>This function first computes gradients of all variables provided. TensorFlow automatically assumes that user-defined variables will be *trained*. Indeed, we defined complete graph, as well as `cost` that depends on `output`, and so on... \n",
    "For each variable, the gradient can be another Tensor, or can represent None when no gradient exists. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # :) \n",
    "\n",
    "X = tf.constant(Xs, name=\"X\")\n",
    "y = tf.constant(Ys, name=\"y\")\n",
    "\n",
    "weights = tf.Variable(tf.random_normal(shape=(2, 1)), name=\"weights\")\n",
    "bias = tf.Variable(0.0, name=\"bias\")\n",
    "net = tf.add(tf.matmul(X, weights), bias, name=\"net\")\n",
    "output = tf.nn.sigmoid(net, name=\"output\")\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(output) + (1-y) * (tf.log(1-output)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.25)\n",
    "training_op = optimizer.minimize(loss=cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all set and will run the graph 20 times. This means that we will calculate current loss 20 times, calculating the gradient each time and adjusting the weights based on learning rate.\n",
    "\n",
    "Because as the documentation states, variable initializers must be run explicitly before other operations (and we surely have declared some), we will use convenient one-line method that will do the trick. This is included in the first line below.\n",
    "\n",
    "Notice how we pass tensors as parameters to the `sess.run` method, so that they are executed and returned. We skip the first variable returned by `.minimize` by a `_` variable, for it will not output any number that is in our interest; we rather want it to perform calculations and result in weights changing. \n",
    "\n",
    "Running the cell below will train our model 20 times, and print current logistic loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0.43907082\n",
      "2 : 0.41127333\n",
      "3 : 0.3878001\n",
      "4 : 0.3677095\n",
      "5 : 0.350308\n",
      "6 : 0.3350761\n",
      "7 : 0.3216183\n",
      "8 : 0.30962878\n",
      "9 : 0.29886782\n",
      "10 : 0.28914505\n",
      "11 : 0.28030756\n",
      "12 : 0.27223098\n",
      "13 : 0.26481354\n",
      "14 : 0.2579709\n",
      "15 : 0.25163263\n",
      "16 : 0.24573967\n",
      "17 : 0.24024187\n",
      "18 : 0.23509651\n",
      "19 : 0.23026697\n",
      "20 : 0.22572167\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(20):\n",
    "        _, current_loss = sess.run([training_op, cost])\n",
    "        print(\"{} : {}\".format(str(epoch+1), str(current_loss)))\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "    https://medium.com/@ouwenhuang/tensorflow-graphs-are-just-protobufs-9df51fc7d08d\n",
    "    https://medium.com/themlblog/getting-started-with-tensorflow-constants-variables-placeholders-and-sessions-80900727b489\n",
    "    https://developers.google.com/protocol-buffers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
